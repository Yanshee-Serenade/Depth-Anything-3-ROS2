# Acknowledgements

This project builds upon the work of several organizations and open-source projects. We extend our gratitude to the following:

## Core Technology

### Depth Anything

This ROS2 wrapper is built around the Depth Anything model developed by the research team. Their work on monocular depth estimation has made this project possible.

- **Paper**: "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data"
- **Repository**: [Depth-Anything](https://github.com/LiheYoung/Depth-Anything)

## Frameworks and Libraries

### ROS2

This project is built on the Robot Operating System 2 (ROS2) framework, developed and maintained by Open Robotics.

- **Website**: [ros.org](https://www.ros.org/)
- **Distribution**: ROS2 Humble Hawksbill

### PyTorch

The deep learning functionality relies on PyTorch, an open-source machine learning framework.

- **Website**: [pytorch.org](https://pytorch.org/)

### OpenCV

Image processing capabilities are provided by OpenCV (Open Source Computer Vision Library).

- **Website**: [opencv.org](https://opencv.org/)

## Contributors

We thank all contributors who have helped improve this project through bug reports, feature suggestions, and code contributions. See the [Contributors](https://github.com/GerdsenAI/GerdsenAI-Depth-Anything-3-ROS2-Wrapper/graphs/contributors) page for a full list.

## GerdsenAI

Developed and maintained by GerdsenAI.

- **GitHub**: [GerdsenAI](https://github.com/GerdsenAI)

## Special Thanks

- The open-source robotics community for their continuous support and feedback
- All users who have tested and provided feedback on this wrapper

---

If you believe your work should be acknowledged here, please open an issue or submit a pull request.